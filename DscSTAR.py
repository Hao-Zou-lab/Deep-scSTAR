# -*- coding: utf-8 -*-
"""Deep_scSTAR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xLGt33dD-2aDUWiEqM4fMJBRbHMclgof
"""

import numpy as np
import torch
import torch.utils.data
import torch.nn.functional as F
import ae as models
import math
import time
from imblearn.over_sampling import RandomOverSampler
import pandas as pd
import torch.nn as nn
import matplotlib.pyplot as plt
import umap
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import seaborn as sns
imblearn_seed = 0

code_size = 512  

def add_noise(x, p=0.2):
    noise = torch.bernoulli(torch.full_like(x, 1-p))
    x_noisy = x * noise
    return x_noisy

class Classifier(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(Classifier, self).__init__()
        self.layer1 = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        x = self.layer1(x)
        return x

def orthogonal_regularization(latent_representation):
    gram = torch.mm(latent_representation, latent_representation.t())
    eye = torch.eye(latent_representation.size(0))
    if latent_representation.is_cuda:
        eye = eye.cuda()
    loss = ((gram - eye) ** 2).sum()
    loss /= latent_representation.size(0) ** 2
    return loss

def training(dataset, nn_paras, plotpath):
   """ Training an autoencoder to remove kinetic bio signal
    Args:
        dataset: dataset of case and ctr
        nn_paras: parameters for neural network training
    Returns:
        model: trained autoencoder
        loss_total_list: list of total loss
        loss_reconstruct_list: list of reconstruction loss
        loss_mse_list: list of classification loss
        loss_ortho_list: list of orthogonal loss
    """ 
   batch_size = nn_paras['batch_size']
   num_epochs = nn_paras['num_epochs']
   num_inputs = nn_paras['num_inputs']
   ae_type = nn_paras['ae_type']
   cuda = nn_paras['cuda']

   # training data for autoencoder, construct a DataLoader for each cluster
   
   gene_exp = dataset['gene_exp'].transpose()
   sample_labels = dataset['sample_labels']
   labels = dataset['labels'].transpose()  
   unique_labels = np.unique(sample_labels)
   
   # Random oversampling based on cell cluster sizes
   data = np.hstack((gene_exp, labels))
   ros = RandomOverSampler(random_state=imblearn_seed)
   data_res, sample_labels = ros.fit_resample(data, sample_labels)

   gene_exp = data_res[:, :gene_exp.shape[1]]
   labels = data_res[:, gene_exp.shape[1]:]

   if cuda:
    torch_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(gene_exp).cuda(), 
        torch.FloatTensor(labels).cuda()  
    )
    print("Using GPU acceleration.")
   else:
    torch_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(gene_exp), 
        torch.FloatTensor(labels) 
    )
    print("Using CPU.")

   data_loader = torch.utils.data.DataLoader(torch_dataset, batch_size=batch_size,
                                          shuffle=True, drop_last=True)


   # create model
   if ae_type == 512:
       model = models.Autoencoder_512(num_inputs=num_inputs)
   elif ae_type == 64:
       model = models.Autoencoder_64(num_inputs=num_inputs)
   else:
       model = models.Autoencoder_512(num_inputs=num_inputs)

   classifier = Classifier(input_dim=code_size, output_dim=2)

   if cuda:
       model.cuda()
       classifier = classifier.cuda()

  # training
   loss_total_list = [] 
   loss_reconstruct_list = []
   loss_mse_list = []
   loss_ortho_list = []

   for epoch in range(1, num_epochs + 1):
        avg_loss, avg_reco_loss, avg_mse_loss, avg_ortho_loss= training_epoch(epoch, model, data_loader, nn_paras, classifier)
        # terminate early if loss is nan
        if math.isnan(avg_reco_loss) or math.isnan(avg_mse_loss):
            return [], model, [], [], []
        loss_total_list.append(avg_loss)
        loss_reconstruct_list.append(avg_reco_loss)
        loss_mse_list.append(avg_mse_loss)
        loss_ortho_list.append(avg_ortho_loss)

   plt.figure(figsize=(12, 6))
   plt.plot(loss_total_list, label='Total Loss')
   plt.xlabel('Epoch')
   plt.ylabel('Loss')
   plt.title('Training Total Loss Over Epochs')
   plt.legend()
   plt.savefig(plotpath)  
   plt.show()

   return model, classifier, loss_total_list, loss_reconstruct_list, loss_mse_list, loss_ortho_list

def training_epoch(epoch, model, data_loader, nn_paras, classifier):
    """ Training an epoch
        Args:
            epoch: number of the current epoch
            model: autoencoder
            data_loader: DataLoader for the dataset
            nn_paras: parameters for neural network training
        Returns:
            avg_total_loss: average total loss of mini-batches
            avg_reco_loss: average reconstruction loss of mini-batches
            avg_mse_loss: average transfer loss of mini-batches
            avg_ortho_loss: average transfer loss of mini-batches
    """
    log_interval = nn_paras['log_interval']
    # load nn parameters
    base_lr = nn_paras['base_lr']
    lr_step = nn_paras['lr_step']
    num_epochs = nn_paras['num_epochs']
    l2_decay = nn_paras['l2_decay']
    gamma = nn_paras['gamma']
    cuda = nn_paras['cuda']

    # step decay of learning rate
    learning_rate = base_lr / math.pow(2, math.floor(epoch / lr_step))
    # regularization parameterbetween two losses
    
    if epoch <= num_epochs / 2:
       gamma_rate = 1 - (0.2 * epoch) / (num_epochs / 2)
    else:
       gamma_rate = 0.8 * (2 - (2 * epoch) / num_epochs)

    if epoch < num_epochs / 8:
       gamma2 = 0
    else:
       gamma2 = 2500
       
    gamma_or = 0.9 / (1 + math.exp(-10 * (epoch - num_epochs / 2) / num_epochs)) + 0.1
    
    
    
    if epoch % log_interval == 0:
        print('{:}, Epoch {}, learning rate {:.3E}, gamma {:.3E}'.format(
                time.asctime(time.localtime()), epoch, learning_rate, gamma_rate * gamma))

    optimizer = torch.optim.Adam([
        {'params': model.encoder.parameters()},
        {'params': model.decoder.parameters()},
    ], lr=learning_rate, weight_decay=l2_decay)

    model.train()

    optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=learning_rate) 

    total_ortho_loss = 0
    total_loss = 0
    total_reco_loss = 0
    total_mse_loss = 0
    num_batches = 0

    for data, labels in data_loader:
        if cuda:
            data = data.cuda()
            labels = labels.cuda()
        noisy_data = add_noise(data)
        code, reconstruct = model(noisy_data)
        pred = classifier(code)

        optimizer.zero_grad()
        optimizer_classifier.zero_grad()

        loss_orthogonal = orthogonal_regularization(code)
        loss_reconstruct = F.mse_loss(reconstruct, data)
        loss_mse = F.mse_loss(pred, labels)
        #mlp
        
        loss =  gamma_rate * gamma * 0.1 * loss_reconstruct +  gamma2 * loss_mse  +  0.01 * gamma_or * loss_orthogonal

        loss.backward()
        optimizer.step()
        optimizer_classifier.step()

        num_batches += 1
        total_loss += loss.data.item()
        total_reco_loss += loss_reconstruct.data.item()
        total_mse_loss += loss_mse.data.item()
        total_ortho_loss += loss_orthogonal.data.item()

    avg_total_loss = total_loss / num_batches
    avg_reco_loss = total_reco_loss / num_batches
    avg_mse_loss = total_mse_loss / num_batches
    avg_ortho_loss = total_ortho_loss / num_batches

    if epoch % log_interval == 0:
        print('Avg_loss {:.12f}\t Avg_reconstruct_loss {:.12f}\t Avg_transfer_loss {:.12f}\t Avg_orthogonal_loss {:.12f}'.format(
        avg_total_loss, avg_reco_loss, avg_mse_loss, avg_ortho_loss))
    return avg_total_loss, avg_reco_loss, avg_mse_loss, avg_ortho_loss

def save_decoder_output(model, dataset, muX, file_path1, file_path2 ,plotpath):
    model.eval()  
    gene_exp = dataset['gene_exp'].transpose()
    labels = dataset['labels'] 
    labels = labels.squeeze()
    with torch.no_grad():  
        if torch.cuda.is_available():  
            gene_exp = torch.FloatTensor(gene_exp).cuda()
        else:
            gene_exp = torch.FloatTensor(gene_exp)
        code, reconstruct = model(gene_exp)
        reconstruct = reconstruct.cpu().numpy()  

    reconstruct = reconstruct + muX
    reconstruct = reconstruct.T

    var_genes = np.var(reconstruct, axis=1)
    top_genes_idx = np.argsort(var_genes)[-500:]
    filtered_data = reconstruct[top_genes_idx, :]

    scaler = StandardScaler()
    standardized_data = scaler.fit_transform(filtered_data.T).T

    pca = PCA(n_components=20)
    pca_result = pca.fit_transform(standardized_data.T)

  
    umap_reducer = umap.UMAP(n_neighbors=30, min_dist=0.3, n_components=2)
    umap_results = umap_reducer.fit_transform(pca_result)
    labels = labels[0, :]

    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=umap_results[:, 0], y=umap_results[:, 1], hue=labels, palette='Spectral')
    plt.xlabel('UMAP 1')
    plt.ylabel('UMAP 2')
    plt.title('UMAP Visualization after PCA of scRNAseq Data')
    plt.legend(title='Label')
    plt.savefig(plotpath)  # Save the plot
    plt.show()

    df = pd.DataFrame(reconstruct, index=dataset['gene_sym'])
    df.columns = pd.MultiIndex.from_arrays([dataset['batch_labels'], df.columns])

    df1 = df.xs(1, axis=1, level=0)
    df2 = df.xs(2, axis=1, level=0)

    df1.to_csv(file_path1)
    df2.to_csv(file_path2)
    print(f"The shape of the output for Case is: {df1.shape}") 
    print(f"The shape of the output for Ctr is: {df2.shape}")  


