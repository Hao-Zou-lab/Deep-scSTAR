# -*- coding: utf-8 -*-
"""ae.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BHYJMpW9FRAZJjgrJznyqUT00rXVhu3_
"""

import torch.nn as nn


def init_weights(m):
    """ initialize weights of fully connected layer """
    if type(m) == nn.Linear:
        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu', a=2.0)  #
        m.bias.data.fill_(0.0)



#--------------------------------------------------------------------

# 
class Encoder_64(nn.Module):
    def __init__(self, num_inputs):
        super(Encoder_64, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(num_inputs, 512),
            #nn.BatchNorm1d(1280),
            nn.LayerNorm(512),
            nn.ELU(alpha=2.0),
            nn.Dropout(0.2),
            nn.Linear(512, 64))
        self.encoder.apply(init_weights)

    def forward(self, x):
        x = self.encoder(x)
        return x

class Decoder_64(nn.Module):
    def __init__(self, num_inputs):
        super(Decoder_64, self).__init__()
        self.decoder = nn.Sequential(
            nn.Linear(64, 512),
            #nn.BatchNorm1d(1280),
            nn.LayerNorm(512),
            nn.ELU(alpha=2.0),
            nn.Dropout(0.1),  # 50% dropout
            nn.Linear(512, num_inputs))
        self.decoder.apply(init_weights)

    def forward(self, x):
        x = self.decoder(x)
        return x


class Autoencoder_64(nn.Module):
    def __init__(self, num_inputs):
        super(Autoencoder_64, self).__init__()
        self.encoder = Encoder_64(num_inputs)
        self.decoder = Decoder_64(num_inputs)

    def forward(self, x):
        code = self.encoder(x)
        x = self.decoder(code)
        return code, x

#-------------------------------------------------------------------

class Encoder_512(nn.Module):
    def __init__(self, num_inputs):
        super(Encoder_512, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(num_inputs, 5120), 
            nn.BatchNorm1d(5120),
            nn.ELU(alpha=2.0),
            nn.Dropout(0.2),   
            nn.Linear(5120, 1024), 
            nn.BatchNorm1d(1024),
            nn.ELU(alpha=2.0),
            nn.Dropout(0.1),  
            nn.Linear(1024, 512)  
        )
        self.encoder.apply(init_weights)

    def forward(self, x):
        x = self.encoder(x)
        return x

class Decoder_512(nn.Module):
    def __init__(self, num_inputs):
        super(Decoder_512, self).__init__()
        self.decoder = nn.Sequential(
            nn.Linear(512, 1024),  
            nn.BatchNorm1d(1024),
            nn.ELU(alpha=2.0),  
            nn.Dropout(0.05),  
            nn.Linear(1024, 5120),
            nn.BatchNorm1d(5120),
            nn.ELU(alpha=2.0), 
            nn.Dropout(0.0),
            nn.Linear(5120, num_inputs),
            nn.ELU(alpha=2.0) 
        )
        self.decoder.apply(init_weights)

    def forward(self, x):
        x = self.decoder(x)
        return x

class Autoencoder_512(nn.Module):
    def __init__(self, num_inputs):
        super(Autoencoder_512, self).__init__()
        self.encoder = Encoder_512(num_inputs)
        self.decoder = Decoder_512(num_inputs)

    def forward(self, x):
        code = self.encoder(x)
        x = self.decoder(code)
        return code, x